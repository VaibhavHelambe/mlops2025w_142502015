{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOxEWk5qzie2",
        "outputId": "9fc940e2-bb82-4416-9f30-0191882a5d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.8.0+cu126)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 — Install dependencies\n",
        "!pip install torchvision torchaudio wandb\n",
        "import torchvision\n",
        "import torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "q71W-wc2zk9u",
        "outputId": "f4bda437-f0da-4d7c-8a04-c1adac7263ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m142502015\u001b[0m (\u001b[33m142502015-indian-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "import wandb\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYT1d9UD0HJF",
        "outputId": "fe6ecb4e-a010-4c88-8088-b2a307628367"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 — Dataloaders factory\n",
        "def get_cifar_loaders(name: str, batch_size=256, num_workers=2, augment=True):\n",
        "    assert name in (\"CIFAR10\", \"CIFAR100\")\n",
        "    if name == \"CIFAR10\":\n",
        "        dataset_class = torchvision.datasets.CIFAR10\n",
        "    else:\n",
        "        dataset_class = torchvision.datasets.CIFAR100\n",
        "\n",
        "    mean = (0.4914, 0.4822, 0.4465)\n",
        "    std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "    train_transforms = [\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ] if augment else [transforms.ToTensor(), transforms.Normalize(mean,std)]\n",
        "\n",
        "    test_transforms = [transforms.ToTensor(), transforms.Normalize(mean,std)]\n",
        "\n",
        "    trainset = dataset_class(root='./data', train=True, download=True, transform=transforms.Compose(train_transforms))\n",
        "    testset  = dataset_class(root='./data', train=False, download=True, transform=transforms.Compose(test_transforms))\n",
        "\n",
        "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "    test_loader  = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return train_loader, test_loader, trainset, testset\n"
      ],
      "metadata": {
        "id": "Wzafrdjy0OVd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 — Model builder (ResNet18 adapted to num_classes)\n",
        "def build_model(num_classes: int, pretrained=False):\n",
        "    model = resnet18(pretrained=pretrained)\n",
        "    # adapt first conv for CIFAR (32x32): change kernel_size=3, stride=1, padding=1\n",
        "    model.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    model.maxpool = torch.nn.Identity()\n",
        "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "NyU-qLjv0Qlu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 — Training/validation utilities + logging helpers\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    losses = []\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    preds = []\n",
        "    targets = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            losses.append(loss.item())\n",
        "            _, p = out.max(1)\n",
        "            correct += (p == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            preds.append(p.cpu().numpy())\n",
        "            targets.append(y.cpu().numpy())\n",
        "    avg_loss = float(np.mean(losses))\n",
        "    acc = correct / total\n",
        "    preds = np.concatenate(preds)\n",
        "    targets = np.concatenate(targets)\n",
        "    return avg_loss, acc, preds, targets\n",
        "\n",
        "def log_confusion_matrix(targets, preds, class_labels, step=None, prefix=\"\"):\n",
        "    # simple confusion matrix logging to W&B (as a table)\n",
        "    cm = confusion_matrix(targets, preds)\n",
        "    # Normalize for readability\n",
        "    cm_norm = cm.astype(float)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        row_sums = cm_norm.sum(axis=1, keepdims=True)\n",
        "        cm_norm = np.divide(cm_norm, row_sums, where=row_sums!=0)\n",
        "    # Log as an image via wandb.plot.confusion_matrix if available\n",
        "    try:\n",
        "        wandb.log({f\"{prefix}confusion_matrix\": wandb.plot.confusion_matrix(probs=None,\n",
        "                                                                           y_true=targets.tolist(),\n",
        "                                                                           preds=preds.tolist(),\n",
        "                                                                           class_names=class_labels)},\n",
        "                  step=step)\n",
        "    except Exception:\n",
        "        # fallback to logging the raw matrix as artifact/table\n",
        "        wandb.log({f\"{prefix}confusion_matrix_array\": cm.tolist()}, step=step)\n"
      ],
      "metadata": {
        "id": "cu-uKY0d0hZO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 — Training loop that supports sequential datasets, and forgetting measurement\n",
        "def run_sequential_experiment(seq, base_seed=42, epochs_per_task=100, batch_size=256, lr=0.01, weight_decay=5e-4):\n",
        "    \"\"\"\n",
        "    seq: list like [\"CIFAR100\", \"CIFAR10\"] in the order to train\n",
        "    \"\"\"\n",
        "    set_seed(base_seed)\n",
        "    # Common initialization: same seed -> same initial weights for both experiments\n",
        "    initial_model = build_model(num_classes=100)  # create model with largest class count\n",
        "    init_state = initial_model.state_dict()\n",
        "\n",
        "    # Initialize a W&B run for this experiment\n",
        "    run_name = \"_then_\".join(seq)\n",
        "    wandb.init(project=\"cifar-sequential-wandb\", name=run_name, config={\n",
        "        \"sequence\": seq,\n",
        "        \"epochs_per_task\": epochs_per_task,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"lr\": lr,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"seed\": base_seed,\n",
        "        \"model\": \"resnet18_cifar\"\n",
        "    })\n",
        "\n",
        "    model = build_model(num_classes=100).to(device)\n",
        "    model.load_state_dict(init_state)  # ensure same start across experiments\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Keep holdouts for both datasets so we can measure forgetting\n",
        "    holdouts = {}  # name -> (dataloader, num_classes, class_labels)\n",
        "    dataloaders = {}  # current dataloaders\n",
        "    datasets_full = {}\n",
        "\n",
        "    for name in (\"CIFAR10\",\"CIFAR100\"):\n",
        "        train_loader, test_loader, trainset, testset = get_cifar_loaders(name, batch_size=batch_size, augment=True)\n",
        "        dataloaders[name] = (train_loader, test_loader)\n",
        "        datasets_full[name] = (trainset, testset)\n",
        "        # class labels\n",
        "        if name==\"CIFAR10\":\n",
        "            labels = [str(i) for i in range(10)]\n",
        "        else:\n",
        "            labels = [str(i) for i in range(100)]\n",
        "        holdouts[name] = (test_loader, len(labels), labels)\n",
        "\n",
        "    # Track checkpoint before any training (to measure initial performance)\n",
        "    performance = {}\n",
        "\n",
        "    # Evaluate initial model on both testsets (after adapting final layer to dataset num_classes)\n",
        "    def eval_on_dataset(model, name):\n",
        "        # Create copy of model and adapt final layer (since we used num_classes=100)\n",
        "        n_classes = 10 if name==\"CIFAR10\" else 100\n",
        "        m = build_model(num_classes=n_classes).to(device)\n",
        "        # copy all weights except final fc (if shapes differ)\n",
        "        sd = model.state_dict()\n",
        "        m_sd = m.state_dict()\n",
        "        # copy compatible keys\n",
        "        for k in m_sd:\n",
        "            if k in sd and sd[k].shape == m_sd[k].shape:\n",
        "                m_sd[k] = sd[k]\n",
        "        m.load_state_dict(m_sd)\n",
        "        return evaluate(m, holdouts[name][0], device)\n",
        "\n",
        "    # Initial eval\n",
        "    for name in seq:\n",
        "        loss, acc, _, _ = eval_on_dataset(model, name)\n",
        "        wandb.log({f\"initial/{name}_loss\": loss, f\"initial/{name}_acc\": acc}, step=0)\n",
        "        performance[f\"initial_{name}\"] = (loss, acc)\n",
        "\n",
        "    global_step = 0\n",
        "    # Now train sequentially\n",
        "    for task_idx, task in enumerate(seq):\n",
        "        train_loader, test_loader = dataloaders[task]\n",
        "        n_classes = 10 if task==\"CIFAR10\" else 100\n",
        "\n",
        "        # If model.fc size != n_classes, replace final layer (fine-tune last layer)\n",
        "        if model.fc.out_features != n_classes:\n",
        "            in_features = model.fc.in_features\n",
        "            model.fc = nn.Linear(in_features, n_classes).to(device)\n",
        "            # note: reinit new layer's params (keeps other weights)\n",
        "        wandb.log({f\"task_started\": task, \"task_index\": task_idx}, step=global_step)\n",
        "        for epoch in range(1, epochs_per_task+1):\n",
        "            model.train()\n",
        "            epoch_losses = []\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for xb, yb in train_loader:\n",
        "                xb = xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                out = model(xb)\n",
        "                loss = criterion(out, yb)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_losses.append(loss.item())\n",
        "                _, p = out.max(1)\n",
        "                correct += (p==yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "                global_step += 1\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss = float(np.mean(epoch_losses))\n",
        "            train_acc = correct/total\n",
        "            val_loss, val_acc, val_preds, val_targets = evaluate(model, test_loader, device)\n",
        "\n",
        "            # Log to W&B\n",
        "            wandb.log({\n",
        "                f\"{task}/train_loss\": train_loss,\n",
        "                f\"{task}/train_acc\": train_acc,\n",
        "                f\"{task}/val_loss\": val_loss,\n",
        "                f\"{task}/val_acc\": val_acc,\n",
        "                \"epoch\": epoch,\n",
        "            }, step=global_step)\n",
        "\n",
        "            # Periodic confusion matrix (less frequent to save bandwidth)\n",
        "            if epoch % 25 == 0 or epoch == epochs_per_task:\n",
        "                class_labels = [str(i) for i in range(n_classes)]\n",
        "                log_confusion_matrix(val_targets, val_preds, class_labels, step=global_step, prefix=f\"{task}/\")\n",
        "\n",
        "        # After finishing this task, evaluate performance on all tasks (measure forgetting)\n",
        "        for other in seq:\n",
        "            loss_o, acc_o, _, _ = eval_on_dataset(model, other)\n",
        "            wandb.log({f\"after_{task}/{other}_loss\": loss_o, f\"after_{task}/{other}_acc\": acc_o}, step=global_step)\n",
        "            performance[f\"after_{task}_{other}\"] = (loss_o, acc_o)\n",
        "\n",
        "        # Save model artifact snapshot\n",
        "        artifact = wandb.Artifact(f\"{run_name}_after_{task}\", type=\"model\")\n",
        "        model_file = f\"model_{run_name}_after_{task}.pth\"\n",
        "        torch.save(model.state_dict(), model_file)\n",
        "        artifact.add_file(model_file)\n",
        "        wandb.log_artifact(artifact)\n",
        "\n",
        "    # Final evaluations and summary\n",
        "    for name in seq:\n",
        "        loss, acc, _, _ = eval_on_dataset(model, name)\n",
        "        wandb.log({f\"final/{name}_loss\": loss, f\"final/{name}_acc\": acc}, step=global_step)\n",
        "        performance[f\"final_{name}\"] = (loss, acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    return performance\n"
      ],
      "metadata": {
        "id": "lJoMkYrG0kRd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 — Run both experiments sequentially in the notebook (this will take time — 200 epochs per experiment total)\n",
        "# If you want to run them in separate Colab sessions / sequentially, comment/uncomment as needed.\n",
        "\n",
        "# EXPERIMENT A: CIFAR100 -> CIFAR10\n",
        "perf_A = run_sequential_experiment([\"CIFAR100\", \"CIFAR10\"], base_seed=42, epochs_per_task=100, batch_size=256, lr=0.05)\n",
        "\n",
        "# EXPERIMENT B: CIFAR10 -> CIFAR100\n",
        "perf_B = run_sequential_experiment([\"CIFAR10\", \"CIFAR100\"], base_seed=42, epochs_per_task=100, batch_size=256, lr=0.05)\n",
        "\n",
        "# Save performance summaries to disk for quick inspection\n",
        "import json\n",
        "with open(\"perf_A.json\",\"w\") as f:\n",
        "    json.dump(perf_A, f)\n",
        "with open(\"perf_B.json\",\"w\") as f:\n",
        "    json.dump(perf_B, f)\n",
        "\n",
        "print(\"Experiments finished. Check W&B project: cifar-sequential-wandb\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "TVwneA2B0oOE",
        "outputId": "073737d7-506e-4b76-c909-b9312dd57d5c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">CIFAR100_then_CIFAR10</strong> at: <a href='https://wandb.ai/142502015-indian-institute-of-technology/cifar-sequential-wandb/runs/t1nnzgsc' target=\"_blank\">https://wandb.ai/142502015-indian-institute-of-technology/cifar-sequential-wandb/runs/t1nnzgsc</a><br> View project at: <a href='https://wandb.ai/142502015-indian-institute-of-technology/cifar-sequential-wandb' target=\"_blank\">https://wandb.ai/142502015-indian-institute-of-technology/cifar-sequential-wandb</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251012_144151-t1nnzgsc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251012_144229-mk7wvlsl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/142502015-indian-institute-of-technology/cifar-sequential-wandb/runs/mk7wvlsl' target=\"_blank\">CIFAR100_then_CIFAR10</a></strong> to <a href='https://wandb.ai/142502015-indian-institute-of-technology/cifar-sequential-wandb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/142502015-indian-institute-of-technology/cifar-sequential-wandb' target=\"_blank\">https://wandb.ai/142502015-indian-institute-of-technology/cifar-sequential-wandb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/142502015-indian-institute-of-technology/cifar-sequential-wandb/runs/mk7wvlsl' target=\"_blank\">https://wandb.ai/142502015-indian-institute-of-technology/cifar-sequential-wandb/runs/mk7wvlsl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1496125575.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# EXPERIMENT A: CIFAR100 -> CIFAR10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mperf_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_sequential_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CIFAR100\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CIFAR10\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_per_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# EXPERIMENT B: CIFAR10 -> CIFAR100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3145656111.py\u001b[0m in \u001b[0;36mrun_sequential_experiment\u001b[0;34m(seq, base_seed, epochs_per_task, batch_size, lr, weight_decay)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ceVgfORz15ek"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}